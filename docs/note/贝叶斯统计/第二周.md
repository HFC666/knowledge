### Metropolis-Hastings

假设我们的目标分布为$g(\theta)$，我们从$q(\theta)$中采样：M-H采样其步骤为：

1. 选择一个初始值$\theta_0$
2. 对于$i=1,\cdots,m$，重复
   1. 从分布$\theta^\star\sim q(\theta^\star|\theta_{i-1})$中采样
   2. 计算$$\alpha = \frac{g(\theta^\star)q(\theta_{i-1}|\theta^\star)}{g(\theta_{i-1})q(\theta^\star|\theta_{i-1})}$$，当$q$为对称的分布时(如正态分布)，则$\alpha = \frac{g(\theta^\star)}{g(\theta_{i-1})}$
   3. 若$\alpha\ge 1$，则接受$\theta^\star$，并把$\theta_i$设置为$\theta^\star$；若$0<\alpha<1$，则以概率$\alpha$接受$\theta^\star$并将$\theta_i$设置为$\theta^\star$，同时以$1-\alpha$的概率拒绝$\theta^\star$并且将$\theta_i$设置为$\theta_{i-1}$。

#### 案例

假设我们的后验分布为：
$$
p(\mu|y_1,\cdots,y_n)\propto \frac{\exp[n(\overline{y}\mu-\mu^2/2)]}{1+\mu^2}
$$
为了数值稳定性，我们计算上述分布的对数：
$$
\log(g(\mu)) = n(\overline{y}\mu-\mu^2/2) - \log(1+\mu^2)
$$

~~~R
lg <- function(mu ,n ,ybar) {
  mu2 <- mu^2
  n * (ybar * mu - mu2 / 2.0) - log(1.0 + mu2)
}

mh <- function(n, ybar, n_iter, mu_init, cand_sd) {
  mu_out <- numeric(n_iter)
  accept <- 0
  mu_now = mu_init
  lg_now = lg(mu=mu_now, n=n, ybar=ybar)
  
  for (i in 1:n_iter) {
    mu_cand <- rnorm(1, mean = mu_now, sd = cand_sd)
    
    lg_cand <- lg(mu = mu_cand, n = n, ybar = ybar)
    lalpha <- lg_cand - lg_now
    alpha <- exp(lalpha)
    
    u <- runif(1)
    if (u < alpha) {
      mu_now <- mu_cand
      accept <- accept + 1
      lg_now <- lg_cand
    }
    mu_out[i] <- mu_now
  }
  list(mu=mu_out, accept=accept/n_iter)
}

# 数据
y <- c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)

ybar <- mean(y)
n<- length(y)

hist(y, freq=FALSE, xlim=c(-1.0, 3.0))
points(y, rep(0.0, n))
points(ybar, 0.0, pch=19)
curve(dt(x, df=1), lty=2, add=TRUE) # 我们的先验为t分布，绘制先验分布

# MH采样
set.seed(43)
# 一般cand_sd即方差越大，接受率越小，而方差过小，每次移动的步伐太短，需要很长时间才能走遍后验分布。
post <- mh(n = n, ybar = ybar, n_iter = 1e3, mu_init = 0.0, cand_sd = 3)
str(post)

library(coda)
traceplot(as.mcmc(post$mu))
~~~

![](image\1.png)

如果我们设置的初始值不当，如：

~~~R
post <- mh(n = n, ybar = ybar, n_iter = 1e3, mu_init = 30.0, cand_sd = 3)
str(post)

library(coda)
traceplot(as.mcmc(post$mu))
~~~

![](image\2.png)

可以看到经过多次迭代后才达到平稳分布，我们需要删除之前未达到平稳的样本。

### JAGS

JAGS为一个用于MCMC的工具。我们下面看如何使用JAGS：

假设我们的模型为：
$$
\begin{aligned}
&y_{i} \mid \mu \stackrel{\mathrm{iid}}{\sim} \mathrm{N}(\mu, 1), \quad i=1, \ldots, n \\
&\mu \sim \mathrm{t}(0,1,1)
\end{aligned}
$$


~~~R
library(rjags)

# 1. Specify the model
mod_string <- "model {
  for (i in 1:n) {
    y[i] ~ dnorm(mu, 1.0/sig2)
  }
  mu ~ dt(0.0, 1.0/1.0, 1)
  sig2 = 1.0
}"

# 2. Set up the model
set.seed(50)

y <- c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)
n <- length(y)

data_jags <- list(y=y, n=n)
params <- c("mu")

inits = function() {
  inits <- list("mu"=0.0)
}

mod <- jags.model(textConnection(mod_string), data = data_jags, inits = inits)


# 3. Run the MCMC sampler

update(mod, 500) # 先迭代500次，达到平稳分布

mod_sim <- coda.samples(model = mod, variable.names = params, n.iter = 1000)

# 4. Post processing
library(coda)
plot(mod_sim)
~~~

![](image\3.png)

### Gibbs Sampling

假设我们的后验分布为$p(\theta,\phi|y)$，Gibbs采样的算法为：

1. 初始$\theta_0,\phi_0$
2. 对于$i=1,\cdots,m$，重复：
   1. 使用$\phi_{i-1}$产生$\theta_i\sim p(\theta|\phi_{i-1},y)$
   2. 使用$\theta_i$产生$\phi_i\sim p(\phi|\theta_i,y)$

例子：

假设我们有如下模型：
$$
\begin{aligned}
y_i|\mu,\sigma^2&\stackrel{\mathrm{iid}}{\sim} \mathcal{N}(\mu,\sigma^2)\\
\mu&\sim \mathcal{N}(\mu_0,\sigma_0^2)\\
\sigma^2&\sim \operatorname{IG}(\nu_0,\beta_0)
\end{aligned}
$$


我们的联合概率分布为：
$$
\begin{aligned}
&p\left(\mu, \sigma^{2} \mid y_{1}, \ldots, y_{n}\right) \propto p\left(y_{1}, \ldots, y_{n} \mid \mu_{1} \sigma^{2}\right) p(\mu) p\left(\sigma^{2}\right)\\
&=\prod_{i=1}^{n}\left[N\left(y_{i} \mid \mu, \sigma^{2}\right)\right] N\left(\mu \mid \mu_{0}, \sigma_{0}^{2}\right) I G\left(\sigma^{2} \mid \nu_{0}, \beta_{0}\right)\\
&=\prod_{i=1}^{n}\left[\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2 \sigma^{2}}\left(y_{i}-\mu^{n}\right)\right)\right] \frac{1}{\sqrt{2 \pi \sigma_{0}^{2}}} \exp \left(-\frac{1}{2 \sigma_{0}^{2}}\left(\mu-\mu_{0}\right)^{2}\right) \frac{\beta_{0}^{\nu_{0}}}{\Gamma\left(N_{0}\right)}\left(\sigma_{0}^{-(\nu_0+1)} \exp \left(\frac{-\beta_{0}}{\sigma^{2}}\right)\right.\\
&\propto\left(\sigma^{2}\right)^{-\frac{n}{2}} \exp \left[-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}\right] \exp \left[-\frac{1}{2 \sigma_{0}^{2}}\left(\mu-\mu_{0}\right)^{2}\right]\left(\sigma^{2}\right)^{-\left(\nu_{0}+1\right)} \exp \left[\frac{-\beta_{0}}{\sigma^{2}}\right]
\end{aligned}
$$


下面我们计算其条件概率作为转移概率：
$$
\begin{aligned}
p\left(\mu \mid \sigma^{2}, y_{1}, \ldots, y_{n}\right) & \propto P\left(\mu, \sigma^{2} \mid y_{1}, \ldots, y_{n}\right) \\
& \propto \exp \left[-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}\right] \exp \left[-\frac{1}{2 \sigma_{0}^{2}}\left(\mu-\mu_{0}\right)^{2}\right] \\
&=\exp \left[-\frac{1}{2}\left(\frac{\sum\left(y_{i}-\mu\right)^{2}}{\sigma^{2}}+\frac{\left(\mu-\mu_{0}\right)^{2}}{\sigma_{0}^{2}}\right)\right] \\
& \propto N\left(\mu \mid \frac{n \bar{y} / \sigma^{2}+\mu_{0} / \sigma_{0}^{2}}{n / \sigma^{2}+1 / \sigma_{0}^{2}}, \frac{1}{n / \sigma^{2}+1 / \sigma_{0}^{2}}\right)
\end{aligned}
$$

$$
\begin{aligned}
p\left(\sigma^{2} \mid \mu, y_{1}, \ldots, y_{n}\right) & \propto p\left(\mu, \sigma^{2} \mid y_{1}, \ldots, y_{n}\right) \\
& \propto\left(\sigma^{2}\right)^{-\frac{n}{2}} \exp \left[-\frac{1}{2 \sigma^{2}} \Sigma\left(y_{i}-\mu\right)^{2}\right]\left(\sigma^{2}\right)^{-\left(\nu_{0}+1\right)} \exp \left[\frac{-\beta_{0}}{\sigma^{2}}\right] \\
&=\left(\sigma^{2}\right)^{-\left(\nu_{0}+\frac{n}{2}+1\right)} \exp \left[-\frac{1}{\sigma^{2}}\left(\beta_{0}+\frac{\sum_{1=1}^{n}\left(y_{i}-\mu\right)^{2}}{2}\right)\right] \\
& \propto I G\left(\sigma^{2} \mid \nu_{0}+\frac{n}{2}, \beta_{0}+\frac{\sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}}{2}\right)
\end{aligned}
$$

~~~R
update_mu = function(n, ybar, sig2, mu_0, sig2_0) {
  sig2_1 = 1.0 / (n / sig2 + 1.0 / sig2_0)
  mu_1 = sig2_1 * (n * ybar / sig2 + mu_0 / sig2_0)
  rnorm(n=1, mean=mu_1, sd=sqrt(sig2_1))
}

update_sig2 = function(n, y, mu, nu_0, beta_0) {
  nu_1 = nu_0 + n / 2.0
  sumsq = sum( (y - mu)^2 ) # vectorized
  beta_1 = beta_0 + sumsq / 2.0
  out_gamma = rgamma(n=1, shape=nu_1, rate=beta_1) # rate for gamma is shape for inv-gamma
  1.0 / out_gamma # reciprocal of a gamma random variable is distributed inv-gamma
}

gibbs = function(y, n_iter, init, prior) {
  ybar = mean(y)
  n = length(y)
  
  ## initialize
  mu_out = numeric(n_iter)
  sig2_out = numeric(n_iter)
  
  mu_now = init$mu
  
  ## Gibbs sampler
  for (i in 1:n_iter) {
    sig2_now = update_sig2(n=n, y=y, mu=mu_now, nu_0=prior$nu_0, beta_0=prior$beta_0)
    mu_now = update_mu(n=n, ybar=ybar, sig2=sig2_now, mu_0=prior$mu_0, sig2_0=prior$sig2_0)
    
    sig2_out[i] = sig2_now
    mu_out[i] = mu_now
  }
  
  cbind(mu=mu_out, sig2=sig2_out)
}

y = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)
ybar = mean(y)
n = length(y)

## prior
prior = list()
prior$mu_0 = 0.0
prior$sig2_0 = 1.0
prior$n_0 = 2.0 # prior effective sample size for sig2
prior$s2_0 = 1.0 # prior point estimate for sig2
prior$nu_0 = prior$n_0 / 2.0 # prior parameter for inverse-gamma
prior$beta_0 = prior$n_0 * prior$s2_0 / 2.0 # prior parameter for inverse-gamma

hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dnorm(x=x, mean=prior$mu_0, sd=sqrt(prior$sig2_0)), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean

set.seed(53)

init = list()
init$mu = 0.0

post = gibbs(y=y, n_iter=1e3, init=init, prior=prior)

library("coda")
plot(as.mcmc(post))
~~~

![](image\4.png)

### Assessing Convergence

#### trace plots

trace plots可以展示样本迭代的过程，下面就是一个很可能收敛的链的例子：

~~~R
set.seed(61)
post0 <- mh(n=n, ybar=ybar, n_iter=10e3, mu_init=0.0, cand_sd=0.9)
coda::traceplot(as.mcmc(post0$mu[-c(1:500)]))
~~~

![](image\5.png)

如果链是平稳的，它就不应该有长期的趋势。链的平均值应该大致相平。不应该像下面的例子那样徘徊：

~~~R
set.seed(61)
post1 <- mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.04)
coda::traceplot(as.mcmc(post1$mu[-c(1:500)]))
~~~

在这种情况下，我们应该迭代更多次：

~~~R
set.seed(61)
post2 = mh(n=n, ybar=ybar, n_iter=100e3, mu_init=0.0, cand_sd=0.04)
coda::traceplot(as.mcmc(post2$mu))
~~~

![](image\6.png)

#### Monte Carlo effective sample size

我们所观察的这两条链之间的一个主要区别是每条链的自相关程度。这里的自相关程度指的是与之前样本之间的关系：

~~~R
coda::autocorr.plot(as.mcmc(post0$mu))
~~~

![](image\7.png)

~~~R
coda::autocorr.diag(as.mcmc(post0$mu))
~~~

~~~R
##             [,1]
## Lag 0  1.0000000
## Lag 1  0.9850078
## Lag 5  0.9213126
## Lag 10 0.8387333
## Lag 50 0.3834563
~~~

自相关很重要，因为它告诉我们在马尔科夫链中有多少信息可用。从高度相关的马尔可夫链中抽样1000次迭代所得到的关于平稳分布的信息，要比我们从平稳分布中独立抽取的1000个样本所得到的信息少。

自相关是计算蒙特卡洛有效样本的一个重要组成。蒙特卡洛有效样本指的是你需要从平稳分布抽取的样本的数量，使这些样本包含的信息与你的马尔科夫链包含的信息相同。

~~~R
coda::effectiveSize(as.mcmc(post2$mu))
~~~

~~~R
##    var1 
## 373.858
~~~

讲样本减少至自相关系数为$0$。这将会给你近似的独立样本。这些样本的数量近似于有效样本。

~~~R
coda::autocorr.plot(as.mcmc(post2$mu), lag.max=500)
~~~

![](image\8.png)

~~~R
thin_interval = 400 # how far apart the iterations are for autocorrelation to be essentially 0.
thin_indx = seq(from=thin_interval, to=length(post2$mu), by=thin_interval)
head(thin_indx)

post2mu_thin = post2$mu[thin_indx]
traceplot(as.mcmc(post2$mu))
~~~

![](image\9.png)

~~~R
traceplot(as.mcmc(post2mu_thin))
~~~

![](image\10.png)

~~~R
coda::autocorr.plot(as.mcmc(post2mu_thin), lag.max=10)
~~~

![](image\11.png)

检查链的蒙特卡罗有效样本大小通常是一个好主意。如果你所寻求的只是一个后验均值估计，那么几百到几千的有效样本量就足够了。然而，如果你想创建一个$95\%$的后验区间，你可能需要成千上万的有效样本来产生分布外边缘的可靠估计。

~~~R
raftery.diag(as.mcmc(post0$mu))
## 
## Quantile (q) = 0.025
## Accuracy (r) = +/- 0.005
## Probability (s) = 0.95 
##                                        
##  Burn-in  Total Lower bound  Dependence
##  (M)      (N)   (Nmin)       factor (I)
##  12       13218 3746         3.53
~~~

~~~R
raftery.diag(as.mcmc(post0$mu), q=0.005, r=0.001, s=0.95)
## 
## Quantile (q) = 0.005
## Accuracy (r) = +/- 0.001
## Probability (s) = 0.95 
## 
## You need a sample size of at least 19112 with these values of q, r and s
~~~

#### Burn-in 

我们还看到了链的初值如何影响链的收敛速度。如果我们的初始值离后验分布的大部分很远，那么这条链可能需要一段时间才能到达那里。

~~~R
set.seed(62)
post3 <- mh(n=n, ybar=ybar, n_iter=500, mu_init=10.0, cand_sd=0.3)
coda::traceplot(as.mcmc(post3$mu))s
~~~

![](image\12.png)

显然，前100次左右的迭代不能反映平稳分布的结果，因此在我们使用这个链进行蒙特卡罗估计之前，应该抛弃它们。这就是所谓的burn-in期。

### Multiple chains, Gelman-Rubin

如果我们想要更确信我们已经收敛到真正的平稳分布，我们可以模拟多个链，每个链有不同的初始值。

~~~R
set.seed(61)

nsim = 500
post1 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=15.0, cand_sd=0.4)
post1$accpt

post2 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=-5.0, cand_sd=0.4)
post2$accpt

post3 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=7.0, cand_sd=0.1)
post3$accpt

post4 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=23.0, cand_sd=0.5)
post4$accpt

post5 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=-17.0, cand_sd=0.4)
post5$accpt

pmc = mcmc.list(as.mcmc(post1$mu), as.mcmc(post2$mu), 
                as.mcmc(post3$mu), as.mcmc(post4$mu), as.mcmc(post5$mu))
str(pmc)

## List of 5
##  $ :Class 'mcmc'  atomic [1:500] 14.8 14 14 13.8 13.8 ...
##   .. ..- attr(*, "mcpar")= num [1:3] 1 500 1
##  $ :Class 'mcmc'  atomic [1:500] -5 -5 -5 -5 -4.89 ...
##   .. ..- attr(*, "mcpar")= num [1:3] 1 500 1
##  $ :Class 'mcmc'  atomic [1:500] 7 7 7 6.94 6.94 ...
##   .. ..- attr(*, "mcpar")= num [1:3] 1 500 1
##  $ :Class 'mcmc'  atomic [1:500] 23 21.9 21.9 21.8 21.8 ...
##   .. ..- attr(*, "mcpar")= num [1:3] 1 500 1
##  $ :Class 'mcmc'  atomic [1:500] -17 -17 -16.9 -16.2 -15.7 ...
##   .. ..- attr(*, "mcpar")= num [1:3] 1 500 1
##  - attr(*, "class")= chr "mcmc.list"
~~~

~~~R
coda::traceplot(pmc)
~~~

![](image\13.png)

我们可以用格尔曼和鲁宾的诊断来支持我们的视觉结果。这个诊断统计量计算链内的可变性，并将其与链间的可变性进行比较。如果所有的链都趋同于平稳分布，链之间的变异应该相对较小，诊断报告的潜在尺度缩减因子应该接近于1。如果这些值远远大于1，那么我们就会得出结论，这些链还没有收敛。

~~~R
coda::gelman.diag(pmc)

## Potential scale reduction factors:
## 
##      Point est. Upper C.I.
## [1,]       1.01       1.02
~~~

~~~R
coda::gelman.plot(pmc)
~~~

![](image\14.png)

~~~R
nburn = 1000 # remember to discard early iterations
post0$mu_keep = post0$mu[-c(1:1000)]
summary(as.mcmc(post0$mu_keep))

## 
## Iterations = 1:9000
## Thinning interval = 1 
## Number of chains = 1 
## Sample size per chain = 9000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##           Mean             SD       Naive SE Time-series SE 
##       0.889449       0.304514       0.003210       0.006295 
## 
## 2. Quantiles for each variable:
## 
##   2.5%    25%    50%    75%  97.5% 
## 0.2915 0.6825 0.8924 1.0868 1.4890
~~~

