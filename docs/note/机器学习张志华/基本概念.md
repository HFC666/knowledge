`Mike Jordan`对机器学习`ML`的定义：

将计算和统计结合起来的学科，和信息论、信号处理、算法、控制论、最优化也有关系。

张志华老师的定义：

机器学习 = 矩阵 + 最优化 + 算法 + 统计

数据即为矩阵，假设有$n$个样本，每个样本$p$个属性。那么数据就可以写为：
$$
X = \begin{bmatrix}
	x_{11} & \dots &x_{1p}\\
	\vdots & \ddots & \vdots \\
	x_{n1} & \dots & x_{np}
	\end{bmatrix}_{n \times p}
	
$$
可以用$\vec{x}_i = (x_{i1},\dots,x_{ip})^T\quad i = 1, \dots, n$表示一个样本，这样$X = [x_i,\dots, x_n]^T$表示数据。

也可以用$\vec{x}_i = (x_{1i},\dots,x_{ni})\quad i = 1, \dots, p$表示一个特征，这样$X = [x_i,\dots,x_p]$表示数据。



$y$为输出变量。

假设$y$为性别，那么这个问题就是一个分类问题，可以假设性别为男时$y=1$，性别为女时$y=-1$。

假设$y$为籍贯，可以设$y$为一个$k$维的向量，假设$y$为籍贯$1$，则$y = [1,0,0\dots,0]^T$。

==分类问题是一种特殊的回归问题。==

假设$y$为每个人的成绩，那么这就是一个回归问题。

以上问题都属于监督问题。



我们将数据分为两块，一部分为$X$，为输入值，$n\times p$的，另一块为$y$，为输出值，$n\times1$的。

### 数据分类

数据分为三类：

+ 训练数据：训练参数
+ 验证数据：用来选择合适的$\lambda$
+ 测试数据



### 回归

假设该问题为一个回归问题，我们采用最小二乘法求解。

最小二乘法原始的公式：
$$
\frac{1}{2}\sum_{i=1}^n(y_i-a^Tx_i)^2
$$
写成矩阵的形式为：
$$
\frac{1}{2}(y-Xa)^T(y-Xa)\\
=\frac{1}{2}||(y-Xa)||_2
$$
对其进行求导：

$$
\frac{\partial L}{\partial a} = -X^T(y-Xa)
$$
假设$X^TX$可逆：

另导数等于$0$，得
$$
a = (X^TX)^{-1}X^Ty
$$
两个矩阵相乘的秩定理：
$$
rank(A*B)\le \min(rank(A),rank(B))
$$
假设$X$为$n\times p$的矩阵，所以$X^TX$为$p\times p$的矩阵，要想$X^TX$可逆，则$p\le n$。

#### 岭回归

如果不可逆，用到正则化的知识，对损失函数进行正则化：
$$
L(f) = \frac{1}{2}(y-Xa)(y-Xa^T)+\frac{\lambda}{2}a^Ta
$$
对其求导，得：
$$
-X^T(y-Xa) + \lambda a
$$
令其等于$0$，得：
$$
a = (X^TX+\lambda I)X^Ty\quad I_{p\times p}\text{为单位矩阵}
$$
此种情况下，$(X^TX+\lambda I)$一定可逆，因为$X^TX$为半正定矩阵（矩阵得转置与矩阵相乘得到得矩阵一定是半正定矩阵），所以$X^TX+\lambda I$为正定矩阵，所以其可逆。

这种方法被称为岭回归。

#### Lasso回归

还有一种正则化的方法，就是将正则化项改为一范数，即
$$
L(f) = \frac{1}{2}(y-Xa)^T(y-Xa) + \lambda\sum_{i=0}^p|a_i|
$$
这样训练出来的模型，$a$中会存在很多$0$值，$0$值对应的元素就会被忽略，相当于删除了一些对结果没有影响的变量。此种方法被称为`lasso`回归。



#### 逻辑斯蒂回归

回到上面所说的那个分类的问题，因为此分类问题中$y\in \{0,1\}$，因此可以将$y$看作是伯努利分布产生的结果，即：
$$
\{y_1,y_2,\cdots,y_n\}\sim  Ber(\alpha)\quad 0<\alpha<1 
$$
之后可以用最大似然估计。
$$
L = \prod_{i=1}^np(y_i) = \prod_{i=1}^n\alpha^{y_1}(1-\alpha)^{1-y_i}
$$
将$\alpha$与$x$联系起来，用`sigmod`函数：$\frac{1}{1+e^{-x}}$。另：
$$
\alpha_i = \frac{1}{1+e^{-x_i^Ta}}
$$


我们一般不会直接对似然函数进行求解，我们一般用似然函数的自然对数，这两种是等价的。

所以：
$$
\ln L = \sum_{i=1}^n(y_i\ln{\alpha_i}+(1-y_i)(\ln{(1-\alpha_i)}))
$$
另：
$$
f(a) = -\ln L
$$
求解$\arg_{a}\min{f}$。这就是逻辑斯蒂回归的另一个角度。

### 无监督算法

无监督算法，上面提到我们的$x\in \mathbb{R}^p$，我们可以寻找一个$z\in \mathbb{R}^q$，使得$q<p$，将$x$映射到$z$，其中$q<\min(n,p)$，从而实现降维。

可以通过一个线性变换将$x$映射到$z$，即$z=B_{q\times p}x_{p\times 1}$，这就是线性降维，如`PCA`。

或者$z=g(x)$，$g$为非线性函数，称为非线性降维。



聚类：clustering

> 在无监督学习中无测试集、训练集、验证集之分，也没有上述提到的$y$



### 半监督

还有半监督

+ 传导学习
+ Active learning



### 分类

1. 参数的：参数的个数不随训练数据的大小而变化，如逻辑斯蒂回归，其参数$a$无论训练数据大小$n$多大其都为$p$维的。
2. 非参数的：模型的参数依赖于训练数据的个数。



1. 判别模型：只需要知道$p(y|x)$，而不关系$x$的分布。
2. 生成模型：关心$x$的分布，比如说$x$是从正态分布中产生的。



1. 贝叶斯派：参数是随机的，将参数看作随机变量，需要给定参数的先验分布，根据数据估计后验分布。
2. 频率学派：核心就是一个优化，根据一个准则如最大似然函数或最小二乘法，对此准则进行优化。



我们将在正则化框架下的损失函数分解为：
$$
L = F(a) + \lambda P(a),F(a)\text{经验风险},P(a)\text{为正则化项}
$$
对数打分原理：我们将函数$F(a)$取$\exp(-F(a))$即可变为似然函数。

正则化项与先验估计有对偶关系，$\exp(-\lambda P(a))$可以看作是先验分布。

这样正则化框架下的损失函数取$\exp(-L)$就可以看作后验概率。