$X = (x_1,x_2,\cdots,x_p)^T$为随机向量。

累积概率函数（分布函数）`cdf`：$F_X(x) = P(X\le x)=P(X_1\le x_1,X_2\le x_2,\cdots,X_p\le x_p)$。



+ 绝对连续分布
+ 离散分布

如果$X$分布是绝对连续的，那么就存在一个概率密度函数$f_X(x)$，使得
$$
F_X(x) = \int_{-\infty}^xf_X(u)du
$$

> 一定要记住$x,u$都为向量，即$du = du_1du_2\cdots du_p$



$D\subseteq \mathbb{R}^p$，则$P(x\in D) = \int_Df(u)du$



对于离散的，有可数个值(有限个或可数无限个)。

`pmf`对应于连续函数的`pdf`，则：
$$
f_X(x_j) = P(X=x_j)
$$
$D\subseteq \mathbb{R}^p$，则$P(x\in D) = \sum_{x_j\in D}f(x_j)$



支撑集$S = \{x\in\mathbb{R}^p\mid f(x)>0\}$，我们的概率在此支撑集上定义。即我们只考虑概率大于$0$的部分。



对于$x = \begin{pmatrix}x_1\\x_2 \end{pmatrix}$，其中$x_1\in\mathbb{R}^k,x_2\in\mathbb{R}^{p-k}$。

边界分布：如果存在$\Pr(X_1\le x_1)=F_X(x_1,\cdots,x_k,\infty_{k+1},\cdots,\infty_{p})$，其中$X\sim f(x)$，则$f_1(x_1) = \int_{-\infty}^{+\infty}f(x)dx_2$为边界分布函数。

条件分布：通过贝叶斯定理：
$$
f_2(x_2|X_1=x_1) = \frac{f(x_1,x_2)}{f(x_1)}
$$
为条件分布。

独立：如果$f(x_2\mid x_1)=f(x_2)$，则说明$x_1,x_2$是独立的。如果$x_1,x_2$是统计独立的，则$f(x_1,x_2)=f(x_1)f(x_2)$。

### 矩

期望：设$X$为cdf为$F_X(x)$的随机**向量**，则期望或均值的一个标量函数$g(X)$。则标量函数的期望为：
$$
\mathbb{E}(g(x)) = \int_{-\infty}^\infty g(x)dF(x) = \begin{cases}\sum_xg(x)f(x)\quad &x\text{为离散的}\\\int_{-\infty}^\infty g(x)f(x)dx\quad&x\text{为连续的}\end{cases}
$$

> 但是并不是所有期望都是有限的，不过之后的讨论我们都假设期望是有限的。

期望的性质：

+ 线性性：$\mathbb{E}(\alpha g_1(x)+\beta g_2(x)) = \alpha\mathbb{E}(g_1(x))+\beta\mathbb{E}(g_2(x))$
+ 设$x = \begin{pmatrix}x_1\\x_2\end{pmatrix}$，则$$\mathbb{E}(g(x_1)) = \int_{-\infty}^\infty g(x_1)f(x_1,x_2)dx_1dx_2=\int_{-\infty}^\infty p(x_1)g(x_1)dx_1$$
+ 假设$x_1\perp x_2$(指相互独立)，则$$\mathbb{E}(g(x_1),g(x_2))=\mathbb{E}(g(x_1))\mathbb{E}(g(x_2))$$
  假设$G=(g_{ij})$为矩阵值函数，则$\mathbb{E}(G(x))=\mathbb{E}(g_{ij}(x))$。为一个矩阵。

假设$X\in \mathbb{R}^p$，则
$$
\mathbb{E}(X) = \mu = \begin{pmatrix}\mu_1\\\vdots\\\mu_p\end{pmatrix}
$$
其中
$$
\mu_1 = \int_{-\infty}^\infty x_if_i(x_i)dx_i
$$
根据期望的线性性，我们有
$$
\mathbb{E}(Ax+b) = A\mathbb{E}(x)+b
$$

协方差：
$$
\operatorname{Cov}(X) = \mathbb{E}((X-\mu)(X-\mu)^T) = \Sigma=V(X)
$$
协方差的性质：
$$
V(a^TX) = a^TV(X)a = a^T\Sigma a=\sum_{i,j=1}^pa_ia_j\sigma_{ij}
$$
由上式，我们可以将$a^TX$看作是一个随机变量，由于随机变量的方差都是大于等于$0$，也就是说，$\forall a,a^T\Sigma a\ge0$，所以$\Sigma$为半正定矩阵，记作$\Sigma\succeq0$。

$$
V(AX+b) = AV(X)A^T
$$
取$\Delta = \operatorname{diag}(\Sigma)$。则我们令
$$
\rho = \Delta^{-\frac{1}{2}}\Sigma\Delta^{-\frac{1}{2}}
$$
为相关系数矩阵。其中
$$
\rho_{ij} = \frac{\sigma_{ij}}{\sigma_{ii}^{\frac{1}{2}}\sigma_{jj}^{\frac{1}{2}}}
$$
上面说的都是关于总体的。

下面我们介绍关于样本的：
假设我们有样本$\{x_1,\cdots,x_n\}\subseteq \mathbb{R}^p$，我们有
$$
\bar{x} = \frac{1}{n}\sum_{i=1}^nx_i
$$
$\bar{x}$为$\mu$的无偏估计，因为
$$
\mathbb{E}(\bar{X}) = \mathbb{E}(\frac{1}{n}\sum_{i=1}^nx_i) = \frac{1}{n}\sum_{i=1}^n\mathbb{E}(x_i) = \frac{1}{n}n\mu=\mu
$$
而协方差为：
$$
	S = \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})(x_i-\bar{x})^T
$$
可以写做：
$$
\sum_{i=1}^n(x_i-\bar{x})(x_i-\bar{x})^T = (x_1-\bar{x},\cdots,x_n-\bar{x})\begin{pmatrix}x_1-\bar{x}\\\vdots\\x_n-\bar{x}\end{pmatrix}
$$
其中
$$
(x_1-\bar{x},\cdots,x_n-\bar{x}) = X^T-\bar{X}\mathrm{1}_n^T
$$
又因为
$$
\bar{X} = \frac{1}{n}\sum_{i=1}^nx_i=\frac{1}{n}X^T\mathrm{1}_n
$$
所以上式可以写作：
$$
(x_1-\bar{x},\cdots,x_n-\bar{x}) = X^T-\frac{1}{n}X^T\mathrm{1}_n\mathrm{1}_n^T = X^T(I_n-\frac{1}{n}1_n1_n^T)
$$
所以
$$
(x_1-\bar{x},\cdots,x_n-\bar{x})\begin{pmatrix}x_1-\bar{x}\\\vdots\\x_n-\bar{x}\end{pmatrix} = X^T(I_n-\frac{1}{n}1_n1_n^T)(I_n-\frac{1}{n}1_n1_n^T)X
$$
我们记
$$
H = (I_n-\frac{1}{n}1_n1_n^T)
$$
我们分析一下矩阵$H$，发现矩阵$H$为幂等矩阵，即$H^2=H$，所以我们可以写作
$$
S=\frac{1}{n-1}X^THX
$$
矩阵$H$也为**中心矩阵**，即其乘以数据矩阵即$HX$的均值为$0$。我们证明一下，假设$Y=HX$，则$\bar{Y}=\frac{1}{n}1_n^TY$，因为$1_n^TH = 1_n^T-1_n^T=0$，所以$\bar{Y}=0$。



